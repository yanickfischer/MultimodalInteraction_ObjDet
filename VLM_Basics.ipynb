{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT-4*) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation \n",
    "- https://platform.openai.com/docs/guides/vision?lang=node\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=image \n",
    "- https://platform.openai.com/docs/api-reference/chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#openAIclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "openAIclient = openai.OpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEXTMODEL = \"gpt-4o-mini\" \n",
    "IMGMODEL= \"gpt-4o-mini\" \n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic call to gpt4 with prompt and image\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message)\n",
    "print(textwrap.fill(response, width=120))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"description\"][\"foreground\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide an exmaple of json format answer, but ideally \n",
    "one could also do it via e.g. pydantic library.\n",
    "\n",
    "Example: \n",
    "```\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str = Field(..., description=\"Position of the person in the environment, e.g., standing, sitting, etc.\")\n",
    "    age: int = Field(..., ge=0, description=\"Age of the person, must be a non-negative integer.\")\n",
    "    activity: str = Field(..., description=\"Activity the person is engaged in, e.g., reading, talking, etc.\")\n",
    "    gender: Literal[\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"] = Field(\n",
    "        ..., description=\"Gender of the person\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int = Field(..., ge=0, description=\"The total number of people in the environment.\")\n",
    "    atmosphere: str = Field(..., description=\"Description of the atmosphere, e.g., calm, lively, etc.\")\n",
    "    hour_of_the_day: int = Field(..., ge=0, le=23, description=\"The hour of the day in 24-hour format.\")\n",
    "    people: List[Person] = Field(..., description=\"List of people and their details.\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_analysis = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give the a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptLLM(prompt = alert_prompt, sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptLLM(prompt = \"Considering the image analysis given\" +str(output_image_analysis)+ \"give me back the coordinates of the 16-years old. If these are not available, infer them form the pic\", sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptLLM(prompt =  \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\", sysprompt= alert_sys_prompt, image = encode_image(img)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "#genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    im,\n",
    "    (\n",
    "        \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\\n \"\n",
    "    ),\n",
    "])\n",
    "response.resolve()\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini can be used to predict bounding boxes based on free form text queries.\n",
    "The model can be prompted to return the boxes in a variety of different formats (dictionary, list, etc). This of course migh need to be parsed. \n",
    "Check: https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing#scrollTo=WFLDgSztv77H\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openai)",
   "language": "python",
   "name": "openai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
